{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLLAMAPool Server Prototype Code\n",
    "import os\n",
    "EndPoint_Queries=os.environ.get('EndPoint_Queries')\n",
    "Endpoint_Results=os.environ.get('Endpoint_Results')\n",
    "EndPoint_NodeStatus=os.environ.get('EndPoint_NodeStatus')\n",
    "\n",
    "def get_queue_name_from_connection_string(connection_string):\n",
    "    # Split the connection string by \";\" and find the EntityPath part\n",
    "    key_value_pairs = connection_string.split(';')\n",
    "    for pair in key_value_pairs:\n",
    "        if pair.startswith('EntityPath='):\n",
    "            # Return the part after 'EntityPath=' which is the queue name\n",
    "            return pair.split('=')[1]\n",
    "    return None\n",
    "\n",
    "#Assert if the environment variables are set\n",
    "if EndPoint_Queries is None:\n",
    "    raise ValueError(\"EndPoint_Queries is not set\")\n",
    "if Endpoint_Results is None:\n",
    "    raise ValueError(\"Endpoint_Results is not set\")\n",
    "if EndPoint_NodeStatus is None:\n",
    "    raise ValueError(\"EndPoint_NodeStatus is not set\")\n",
    "print(\"Environment Variables are set OK\")\n",
    "\n",
    "#Get the queue names from the connection strings\n",
    "QueueName_Queries = get_queue_name_from_connection_string(EndPoint_Queries) \n",
    "QueueName_Results = get_queue_name_from_connection_string(Endpoint_Results)\n",
    "QueueName_NodeStatus = get_queue_name_from_connection_string(EndPoint_NodeStatus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Busines Classes (Requests/Results)\n",
    "\n",
    "import json\n",
    "from typing import List\n",
    "import uuid\n",
    "from azure.servicebus import ServiceBusClient, ServiceBusMessage\n",
    "\n",
    "class LLMRequest:\n",
    "    def __init__(self,Model:str=\"\",systemMessage:str=\"\",query:str=\"\"):\n",
    "        self.UUID=str(uuid.uuid4())\n",
    "        self.Model=Model\n",
    "        self.systemMessage=systemMessage\n",
    "        self.query=query\n",
    "        self.Messages=[{'role': 'system', 'content': systemMessage},\n",
    "                       {'role': 'user', 'content': query}]\n",
    "\n",
    "    def to_json(self):\n",
    "        return self.__dict__\n",
    "    \n",
    "    def from_json(self,json_str):\n",
    "        self.__dict__=json.loads(json_str)\n",
    "        \n",
    "class LLMResult:\n",
    "    def __init__(self,UUID:str=\"\",result:str=\"\",errorMsg:str=\"\",timeDelta:str=\"\"):\n",
    "        self.UUID=UUID\n",
    "        self.result=result\n",
    "        self.errorMsg=errorMsg\n",
    "        self.HasError=errorMsg!=\"\"\n",
    "        self.timeDelta=timeDelta\n",
    "\n",
    "    def to_json(self):\n",
    "        return self.__dict__\n",
    "    \n",
    "    def from_json(self,json_str):\n",
    "        self.__dict__=json.loads(json_str)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NodeStatus Class - tracks the health of the node, available models and manages communicating state\n",
    "import socket\n",
    "from ollama import Client\n",
    "\n",
    "class NodeStatus():\n",
    "    \n",
    "    def __init__(self,Ollamahost:str,QueueName:str,ConnectionString:str):\n",
    "        self.__Client__=ServiceBusClient.from_connection_string(ConnectionString)\n",
    "        self.__sender__ = self.__Client__.get_queue_sender(queue_name=QueueName)       \n",
    "        self.Host=socket.gethostname()\n",
    "        self.Ollamahost=Ollamahost\n",
    "        self.Models=[]\n",
    "        self.Status=\"Initializing...\"\n",
    "        self.Message=\"\"\n",
    "        self.LastQueryTime=0\n",
    "        self.Client=None\n",
    "\n",
    "    def to_json(self):\n",
    "        return {\"Host\":self.Host,\n",
    "                \"OllamaHost\":self.Ollamahost,\n",
    "                \"Status\":self.Status,\n",
    "                \"Message\":self.Message,\n",
    "                \"Models\":self.Models,\n",
    "                \"LastQueryTime\":self.LastQueryTime}\n",
    "\n",
    "\n",
    "    def from_json(self,json_str):\n",
    "        self.__dict__=json.loads(json_str)\n",
    "\n",
    "    def SyncStatus(self):\n",
    "        try:\n",
    "            message = ServiceBusMessage(json.dumps(self.to_json()))\n",
    "            self.__sender__.send_messages(message)\n",
    "            print(\"Sent status to Queue\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error Sending Status to Queue: {str(e)}\")        \n",
    "        \n",
    "    def SetStatus(self,Status:str,Message:str):\n",
    "        print(f\"Status: {Message}\")\n",
    "        self.Status=Status\n",
    "        self.Message=Message\n",
    "        self.SyncStatus()\n",
    "        \n",
    "    def SetErrorStatus(self,Message:str):\n",
    "        print(f\"Error: {Message}\")\n",
    "        self.Status=\"Error\"\n",
    "        self.Message=Message \n",
    "        self.SyncStatus()  \n",
    "    \n",
    "    def HasModel(self,Model:str)->bool:\n",
    "        modelLatest=Model.lower()+\":latest\"\n",
    "        return (Model in self.Models) or (modelLatest in self.Models)\n",
    "    \n",
    "    #Connects to OLLAMA server and gets the list of models\n",
    "    def Connect(self):\n",
    "        try:\n",
    "            self.Client=Client(host=self.Ollamahost)\n",
    "            models=self.Client.list()\n",
    "            self.Models=[model[\"name\"] for model in models[\"models\"]]\n",
    "            self.SyncStatus()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.SetErrorStatus(str(e))\n",
    "            self.Models=[]\n",
    "            return False\n",
    "\n",
    "#Test code\n",
    "#node=NodeStatus(\"http://localhost:11434\",QueueName_NodeStatus,EndPoint_NodeStatus)\n",
    "\n",
    "# models=node.Connect()\n",
    "# node.SetStatus(\"Ready\",\"Connected to OLLAMA Server\")\n",
    "# # for model in models:\n",
    "# #     print(model)\n",
    "# node.HasModel(\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ollama Processing Code\n",
    "import datetime\n",
    "from ollama import Client\n",
    "\n",
    "#Handles running the LLMRequest and posting the result back to the results queue\n",
    "class LLMRequestServer():\n",
    "    \n",
    "    def __init__(self,node:NodeStatus,ResultsConnectionString:str,ResultsQueueName:str):\n",
    "        self.Client=Client(host=node.Ollamahost)\n",
    "        self.ResultsConnectionString=ResultsConnectionString\n",
    "        self.ResultsQueueName=ResultsQueueName\n",
    "        self.node=node\n",
    "\n",
    "    #Post to a service bus queue    \n",
    "    def AzurePost_ServiceBus(self,json_payload):\n",
    "        try:\n",
    "            with ServiceBusClient.from_connection_string(self.ResultsConnectionString) as client:\n",
    "                sender = client.get_queue_sender(queue_name=self.ResultsQueueName)\n",
    "                message = ServiceBusMessage(json.dumps(json_payload))\n",
    "                with sender:\n",
    "                    sender.send_messages(message)\n",
    "                    print(f\"Queued message: {json_payload['UUID']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error Sending to Queue: {str(e)}\")\n",
    "\n",
    "    def ProcessLLMRequest(self,request:LLMRequest)->LLMResult:\n",
    "        try:\n",
    "            \n",
    "            #Download the model if it is not already downloaded\n",
    "            if not self.node.HasModel(request.Model):\n",
    "                self.node.SetStatus(\"Downloading\",f\"Downloading Model {request.Model}\")\n",
    "                self.Client.pull(request.Model)\n",
    "                self.node.SetStatus(\"Ready\",f\"Model {request.Model} Downloaded\")\n",
    "            \n",
    "            timerStart=datetime.datetime.now()\n",
    "            self.node.SetStatus(\"Running\",f\"Processing{request.UUID}\")    \n",
    "            ret = self.Client.chat(\n",
    "                model=request.Model,\n",
    "                messages=request.Messages,\n",
    "                stream=False)\n",
    "        \n",
    "            #get result/timing and post back to results queue\n",
    "            timerEnd=datetime.datetime.now()\n",
    "            timeDelta=timerEnd-timerStart\n",
    "            result=LLMResult(UUID=request.UUID,result=ret,timeDelta=str(timeDelta))\n",
    "            self.AzurePost_ServiceBus(result.to_json())\n",
    "            self.node.LastQueryTime=timeDelta.total_seconds()\n",
    "            self.node.SetStatus(\"Finsihed\",f\"Processing{request.UUID}\")    \n",
    "            \n",
    "        except Exception as e:\n",
    "            #Print Exeption Type and Message\n",
    "            print(\"Exception!-------------------------------------------------\")\n",
    "            print(e)\n",
    "            self.node.SetErrorStatus(f\"Error Processing{request.UUID}: {str(e)}\")\n",
    "            print(\"Exception!-------------------------------------------------\\n\")\n",
    "            result=LLMResult(UUID=request.UUID,errorMsg=str(e))\n",
    "            self.AzurePost_ServiceBus(result.to_json())\n",
    "\n",
    "            \n",
    "        return result\n",
    "    \n",
    "#node=NodeStatus(\"http://localhost:11434\",QueueName_NodeStatus,EndPoint_NodeStatus)\n",
    "#llmserver=LLMRequestServer(node,Endpoint_Results,QueueName_Results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Shut Down Complete\n",
      "Sent status to Queue\n"
     ]
    }
   ],
   "source": [
    "#Main Message Handling Loop\n",
    "import time\n",
    "from typing import List\n",
    "from azure.servicebus import ServiceBusClient, ServiceBusMessage\n",
    "import signal\n",
    "\n",
    "#Startup\n",
    "node=NodeStatus(\"http://localhost:11434\",QueueName_NodeStatus,EndPoint_NodeStatus)\n",
    "llmserver=LLMRequestServer(node,Endpoint_Results,QueueName_Results)\n",
    "node.SetStatus(\"Ready\",\"Waiting for Queries\")\n",
    "running=True\n",
    "\n",
    "def handle_signal(signal_number, frame):\n",
    "    global running\n",
    "    print(\"Signal received:\", signal_number)\n",
    "    running = False\n",
    "\n",
    "# Register the signal handler\n",
    "signal.signal(signal.SIGINT, handle_signal)\n",
    "signal.signal(signal.SIGTERM, handle_signal)\n",
    "\n",
    "# Create a Service Bus client\n",
    "servicebus_client = ServiceBusClient.from_connection_string(conn_str=EndPoint_Queries)\n",
    "\n",
    "def receive_messages_from_queue(node:NodeStatus):\n",
    "    # Create a receiver for the queue\n",
    "    try:\n",
    "        with servicebus_client.get_queue_receiver(queue_name=QueueName_Queries) as receiver:\n",
    "            print(\"Receiving messages from the queue...\")\n",
    "            received_msgs = receiver.receive_messages(max_message_count=1, max_wait_time=30)\n",
    "            for msg in received_msgs:\n",
    "                print(f\"Received message: {str(msg)}\")\n",
    "                receiver.complete_message(msg)\n",
    "                llmRequest=LLMRequest()\n",
    "                llmRequest.from_json(str(msg))\n",
    "                result=llmserver.ProcessLLMRequest(llmRequest)\n",
    "    except Exception as e:\n",
    "        print(f\"Error Receiving from Queue: {str(e)}\")\n",
    "        node.SetErrorStatus(f\"Error Receiving from Queue: {str(e)}\")\n",
    "        time.sleep(5)\n",
    "        return\n",
    "            \n",
    "#Main loop\n",
    "try:\n",
    "    while running:\n",
    "        receive_messages_from_queue(node)\n",
    "except Exception as e:\n",
    "    print(f\"Error in Main Loop: {str(e)}\")\n",
    "finally:\n",
    "    node.SetStatus(\"Shutdown\",\"Shut Down Complete\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorchCUDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
