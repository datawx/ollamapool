{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLLAMAPool WebUI Monitoring\n",
    "import os\n",
    "EndPoint_NodeStatus=os.environ.get('EndPoint_NodeStatus')\n",
    "\n",
    "#Assert if the environment variables are set\n",
    "if EndPoint_NodeStatus is None:\n",
    "    raise ValueError(\"EndPoint_NodeStatus is not set\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trimmed down NodeStatus class\n",
    "import json\n",
    "class NodeStatus():\n",
    "    \n",
    "    def from_json(self,json_str):\n",
    "        self.__dict__=json.loads(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Message Handling Loop\n",
    "import time\n",
    "from typing import List\n",
    "from azure.servicebus import ServiceBusClient, ServiceBusMessage\n",
    "\n",
    "# Replace with your connection string and queue name\n",
    "queue_name = \"node-status\"\n",
    "hosts_Status={}\n",
    "\n",
    "# Create a Service Bus client\n",
    "servicebus_client = ServiceBusClient.from_connection_string(conn_str=EndPoint_NodeStatus)\n",
    "\n",
    "def receive_messages_from_queue():\n",
    "    # Create a receiver for the queue\n",
    "    with servicebus_client.get_queue_receiver(queue_name=queue_name) as receiver:\n",
    "        print(\"Receiving messages from the queue...\")\n",
    "        \n",
    "        # Receive messages in a batch, you can specify max_message_count as needed\n",
    "        received_msgs = receiver.receive_messages(max_message_count=20, max_wait_time=30)\n",
    "\n",
    "        for msg in received_msgs:\n",
    "            # Print the message payload\n",
    "            print(f\"Received message: {str(msg)}\")\n",
    "            \n",
    "            #Update node status\n",
    "            nodeStatus=NodeStatus()\n",
    "            nodeStatus.from_json(str(msg))\n",
    "            hosts_Status[nodeStatus.Host]=nodeStatus\n",
    "            #print(f\"Hosts Status: {hosts_Status}\")\n",
    "            #print(nodeStatus.to_json())\n",
    "            \n",
    "            # Accept the message to remove it from the queue\n",
    "            receiver.complete_message(msg)\n",
    "            \n",
    "\n",
    "#receive_messages_from_queue()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Web Dash\n",
    "from flask import Flask, render_template\n",
    "from threading import Thread\n",
    "import json\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Example hosts_Status dictionary\n",
    "# hosts_Status = {\n",
    "#     \"host1\": {\"Host\": \"host1\", \"OllamaHost\": \"ollama1\", \"Status\": \"Online\", \"Message\": \"All good\", \"Models\": [\"model1\", \"model2\"], \"LastQueryTime\": 1234567890},\n",
    "#     \"host2\": {\"Host\": \"host2\", \"OllamaHost\": \"ollama2\", \"Status\": \"Offline\", \"Message\": \"Error\", \"Models\": [\"model3\"], \"LastQueryTime\": 1234567890}\n",
    "# }\n",
    "\n",
    "@app.route('/')\n",
    "def show_hosts():\n",
    "    return render_template('hosts.html', hosts_status=hosts_Status)\n",
    "\n",
    "# Thread to run Flask in Jupyter\n",
    "def run_app():\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "\n",
    "# Running Flask in a separate thread\n",
    "flask_thread = Thread(target=run_app)\n",
    "flask_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.1.102:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [26/Sep/2024 12:39:19] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Sep/2024 12:39:19] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [26/Sep/2024 12:39:22] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Sep/2024 12:39:36] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Sep/2024 12:39:50] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Sep/2024 12:40:15] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Sep/2024 12:43:08] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Sep/2024 12:44:38] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Sep/2024 12:44:44] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Sep/2024 12:44:49] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Sep/2024 12:45:04] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Sep/2024 12:45:05] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Sep/2024 12:45:16] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Sep/2024 12:48:29] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Sep/2024 12:48:46] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Sep/2024 12:48:53] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Sep/2024 12:49:16] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Sep/2024 12:51:26] \"GET / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"6c170f511d09\", \"OllamaHost\": \"http://host.docker.internal:11434\", \"Status\": \"Shutdown\", \"Message\": \"Shut Down Complete\", \"Models\": [\"mistral-small:latest\", \"llama3.1:latest\"], \"LastQueryTime\": 4.819281}\n",
      "Received message: {\"Host\": \"6c170f511d09\", \"OllamaHost\": \"http://host.docker.internal:11434\", \"Status\": \"Ready\", \"Message\": \"Connected to Ollama Server\", \"Models\": [\"mistral-small:latest\", \"llama3.1:latest\"], \"LastQueryTime\": 0}\n",
      "Received message: {\"Host\": \"6c170f511d09\", \"OllamaHost\": \"http://host.docker.internal:11434\", \"Status\": \"Shutdown\", \"Message\": \"Shut Down Complete\", \"Models\": [\"mistral-small:latest\", \"llama3.1:latest\"], \"LastQueryTime\": 0}\n",
      "Received message: {\"Host\": \"3372e767769b\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Ready\", \"Message\": \"Connected to Ollama Server\", \"Models\": [], \"LastQueryTime\": 0}\n",
      "Received message: {\"Host\": \"3372e767769b\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Shutdown\", \"Message\": \"Shut Down Complete\", \"Models\": [], \"LastQueryTime\": 0}\n",
      "Received message: {\"Host\": \"df6a6f9481bf\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Ready\", \"Message\": \"Connected to Ollama Server\", \"Models\": [], \"LastQueryTime\": 0}\n",
      "Received message: {\"Host\": \"df6a6f9481bf\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Downloading\", \"Message\": \"Downloading Model llama3.1\", \"Models\": [], \"LastQueryTime\": 0}\n",
      "Received message: {\"Host\": \"df6a6f9481bf\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Error\", \"Message\": \"Error Processing29671910-6ba3-4090-8de3-901775d21d1f: Server disconnected without sending a response.\", \"Models\": [], \"LastQueryTime\": 0}\n",
      "Received message: {\"Host\": \"df6a6f9481bf\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Shutdown\", \"Message\": \"Shut Down Complete\", \"Models\": [], \"LastQueryTime\": 0}\n",
      "Received message: {\"Host\": \"df6a6f9481bf\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Ready\", \"Message\": \"Connected to Ollama Server\", \"Models\": [], \"LastQueryTime\": 0}\n",
      "Received message: {\"Host\": \"df6a6f9481bf\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Downloading\", \"Message\": \"Downloading Model llama3.1\", \"Models\": [], \"LastQueryTime\": 0}\n",
      "Received message: {\"Host\": \"df6a6f9481bf\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Error\", \"Message\": \"Error Processingbbb28619-0739-48c5-874b-fab4ef6b523b: Server disconnected without sending a response.\", \"Models\": [], \"LastQueryTime\": 0}\n",
      "Received message: {\"Host\": \"df6a6f9481bf\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Shutdown\", \"Message\": \"Shut Down Complete\", \"Models\": [], \"LastQueryTime\": 0}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Ready\", \"Message\": \"Connected to Ollama Server\", \"Models\": [], \"LastQueryTime\": 0}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Downloading\", \"Message\": \"Downloading Model llama3.1\", \"Models\": [], \"LastQueryTime\": 0}\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Ready\", \"Message\": \"Model llama3.1 Downloaded\", \"Models\": [], \"LastQueryTime\": 0}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processingf4f1ee89-d8b4-4706-925d-940a64e1b906\", \"Models\": [], \"LastQueryTime\": 0}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processingf4f1ee89-d8b4-4706-925d-940a64e1b906\", \"Models\": [], \"LastQueryTime\": 8.293466}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Downloading\", \"Message\": \"Downloading Model llama3.1\", \"Models\": [], \"LastQueryTime\": 8.293466}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Ready\", \"Message\": \"Model llama3.1 Downloaded\", \"Models\": [], \"LastQueryTime\": 8.293466}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processing2599bc0e-7eda-48b0-a37e-58af5758a80a\", \"Models\": [], \"LastQueryTime\": 8.293466}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing2599bc0e-7eda-48b0-a37e-58af5758a80a\", \"Models\": [], \"LastQueryTime\": 4.934693}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Downloading\", \"Message\": \"Downloading Model llama3.1\", \"Models\": [], \"LastQueryTime\": 4.934693}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Ready\", \"Message\": \"Model llama3.1 Downloaded\", \"Models\": [], \"LastQueryTime\": 4.934693}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processing9a883795-cbc9-4bbd-8858-2dc28cd08843\", \"Models\": [], \"LastQueryTime\": 4.934693}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing9a883795-cbc9-4bbd-8858-2dc28cd08843\", \"Models\": [], \"LastQueryTime\": 5.182237}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Downloading\", \"Message\": \"Downloading Model llama3.1\", \"Models\": [], \"LastQueryTime\": 5.182237}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Ready\", \"Message\": \"Model llama3.1 Downloaded\", \"Models\": [], \"LastQueryTime\": 5.182237}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processing782295d1-2824-4c77-b531-50dab3774a45\", \"Models\": [], \"LastQueryTime\": 5.182237}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing782295d1-2824-4c77-b531-50dab3774a45\", \"Models\": [], \"LastQueryTime\": 4.796595}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Downloading\", \"Message\": \"Downloading Model llama3.1\", \"Models\": [], \"LastQueryTime\": 4.796595}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Ready\", \"Message\": \"Model llama3.1 Downloaded\", \"Models\": [], \"LastQueryTime\": 4.796595}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processinge6f530bc-42d6-4d6d-8c4b-c95cbab2fc83\", \"Models\": [], \"LastQueryTime\": 4.796595}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processinge6f530bc-42d6-4d6d-8c4b-c95cbab2fc83\", \"Models\": [], \"LastQueryTime\": 6.092358}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Downloading\", \"Message\": \"Downloading Model llama3.1\", \"Models\": [], \"LastQueryTime\": 6.092358}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Ready\", \"Message\": \"Model llama3.1 Downloaded\", \"Models\": [], \"LastQueryTime\": 6.092358}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processing6d1c3da9-31ce-4cfb-b269-5dc04d375655\", \"Models\": [], \"LastQueryTime\": 6.092358}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing6d1c3da9-31ce-4cfb-b269-5dc04d375655\", \"Models\": [], \"LastQueryTime\": 7.347018}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Downloading\", \"Message\": \"Downloading Model llama3.1\", \"Models\": [], \"LastQueryTime\": 7.347018}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Ready\", \"Message\": \"Model llama3.1 Downloaded\", \"Models\": [], \"LastQueryTime\": 7.347018}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processing36874288-a22a-400e-a904-c31dc14f57f3\", \"Models\": [], \"LastQueryTime\": 7.347018}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing36874288-a22a-400e-a904-c31dc14f57f3\", \"Models\": [], \"LastQueryTime\": 5.174694}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Downloading\", \"Message\": \"Downloading Model llama3.1\", \"Models\": [], \"LastQueryTime\": 5.174694}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Ready\", \"Message\": \"Model llama3.1 Downloaded\", \"Models\": [], \"LastQueryTime\": 5.174694}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processinge8c4d438-8c50-4f8b-9bf5-42aec133d5af\", \"Models\": [], \"LastQueryTime\": 5.174694}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processinge8c4d438-8c50-4f8b-9bf5-42aec133d5af\", \"Models\": [], \"LastQueryTime\": 5.528606}\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Downloading\", \"Message\": \"Downloading Model llama3.1\", \"Models\": [], \"LastQueryTime\": 5.528606}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Ready\", \"Message\": \"Model llama3.1 Downloaded\", \"Models\": [], \"LastQueryTime\": 5.528606}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processing03a7c262-6312-4668-bd82-299b571054f4\", \"Models\": [], \"LastQueryTime\": 5.528606}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing03a7c262-6312-4668-bd82-299b571054f4\", \"Models\": [], \"LastQueryTime\": 4.400271}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Downloading\", \"Message\": \"Downloading Model llama3.1\", \"Models\": [], \"LastQueryTime\": 4.400271}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Ready\", \"Message\": \"Model llama3.1 Downloaded\", \"Models\": [], \"LastQueryTime\": 4.400271}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processingbc76b89e-4f16-4fb7-993e-1f6c9973e90f\", \"Models\": [], \"LastQueryTime\": 4.400271}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processingbc76b89e-4f16-4fb7-993e-1f6c9973e90f\", \"Models\": [], \"LastQueryTime\": 6.083878}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Downloading\", \"Message\": \"Downloading Model llama3.1\", \"Models\": [], \"LastQueryTime\": 6.083878}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Ready\", \"Message\": \"Model llama3.1 Downloaded\", \"Models\": [], \"LastQueryTime\": 6.083878}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processing14d496e5-9d3b-48a6-bf9a-91e0d6b2345f\", \"Models\": [], \"LastQueryTime\": 6.083878}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing14d496e5-9d3b-48a6-bf9a-91e0d6b2345f\", \"Models\": [], \"LastQueryTime\": 6.278669}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Downloading\", \"Message\": \"Downloading Model llama3.1\", \"Models\": [], \"LastQueryTime\": 6.278669}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Ready\", \"Message\": \"Model llama3.1 Downloaded\", \"Models\": [], \"LastQueryTime\": 6.278669}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processingb2799f79-b599-4ad5-b421-fa511d8e1db7\", \"Models\": [], \"LastQueryTime\": 6.278669}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processingb2799f79-b599-4ad5-b421-fa511d8e1db7\", \"Models\": [], \"LastQueryTime\": 5.520329}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Downloading\", \"Message\": \"Downloading Model llama3.1\", \"Models\": [], \"LastQueryTime\": 5.520329}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Ready\", \"Message\": \"Model llama3.1 Downloaded\", \"Models\": [], \"LastQueryTime\": 5.520329}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processing1dcfe1fa-0e2e-4dd6-bb75-e31fc99fa9e3\", \"Models\": [], \"LastQueryTime\": 5.520329}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing1dcfe1fa-0e2e-4dd6-bb75-e31fc99fa9e3\", \"Models\": [], \"LastQueryTime\": 5.911673}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Downloading\", \"Message\": \"Downloading Model llama3.1\", \"Models\": [], \"LastQueryTime\": 5.911673}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Ready\", \"Message\": \"Model llama3.1 Downloaded\", \"Models\": [], \"LastQueryTime\": 5.911673}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processing407bed53-5ec0-4816-8f58-8035fdcfb599\", \"Models\": [], \"LastQueryTime\": 5.911673}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing407bed53-5ec0-4816-8f58-8035fdcfb599\", \"Models\": [], \"LastQueryTime\": 6.016646}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Downloading\", \"Message\": \"Downloading Model llama3.1\", \"Models\": [], \"LastQueryTime\": 6.016646}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Ready\", \"Message\": \"Model llama3.1 Downloaded\", \"Models\": [], \"LastQueryTime\": 6.016646}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processing7a9c4840-1107-48b0-b0b0-3acd92f7f675\", \"Models\": [], \"LastQueryTime\": 6.016646}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing7a9c4840-1107-48b0-b0b0-3acd92f7f675\", \"Models\": [], \"LastQueryTime\": 4.974047}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Downloading\", \"Message\": \"Downloading Model llama3.1\", \"Models\": [], \"LastQueryTime\": 4.974047}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Ready\", \"Message\": \"Model llama3.1 Downloaded\", \"Models\": [], \"LastQueryTime\": 4.974047}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processing0039ce35-93f8-4925-875a-b8c07e38916c\", \"Models\": [], \"LastQueryTime\": 4.974047}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing0039ce35-93f8-4925-875a-b8c07e38916c\", \"Models\": [], \"LastQueryTime\": 4.873336}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Downloading\", \"Message\": \"Downloading Model llama3.1\", \"Models\": [], \"LastQueryTime\": 4.873336}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Ready\", \"Message\": \"Model llama3.1 Downloaded\", \"Models\": [], \"LastQueryTime\": 4.873336}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processingf5143d7c-788a-42fc-93f8-baba91bd8c07\", \"Models\": [], \"LastQueryTime\": 4.873336}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processingf5143d7c-788a-42fc-93f8-baba91bd8c07\", \"Models\": [], \"LastQueryTime\": 5.559898}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Downloading\", \"Message\": \"Downloading Model llama3.1\", \"Models\": [], \"LastQueryTime\": 5.559898}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Ready\", \"Message\": \"Model llama3.1 Downloaded\", \"Models\": [], \"LastQueryTime\": 5.559898}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processingc2405d26-519b-4fb1-9957-91bec9bb6758\", \"Models\": [], \"LastQueryTime\": 5.559898}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processingc2405d26-519b-4fb1-9957-91bec9bb6758\", \"Models\": [], \"LastQueryTime\": 4.678733}\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Shutdown\", \"Message\": \"Shut Down Complete\", \"Models\": [], \"LastQueryTime\": 4.678733}\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"fb03db0137f1\", \"OllamaHost\": \"http://cpuworker1:11434\", \"Status\": \"Ready\", \"Message\": \"Connected to Ollama Server\", \"Models\": [], \"LastQueryTime\": 0}\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Ready\", \"Message\": \"Connected to Ollama Server\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 0}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"fb03db0137f1\", \"OllamaHost\": \"http://cpuworker1:11434\", \"Status\": \"Downloading\", \"Message\": \"Downloading Model llama3.1\", \"Models\": [], \"LastQueryTime\": 0}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processing1efc3913-fbdc-4e34-bacf-bad71eb6961e\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 0}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing1efc3913-fbdc-4e34-bacf-bad71eb6961e\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 7.558568}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processing5621312a-7cb4-4b89-b344-0f1fa5ade977\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 7.558568}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing5621312a-7cb4-4b89-b344-0f1fa5ade977\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 5.623239}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processing23ba843b-3769-4522-a11c-5b47a113f257\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 5.623239}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing23ba843b-3769-4522-a11c-5b47a113f257\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 4.998685}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processinge14202e3-dd4b-4250-bf27-e2ce5efa0ffe\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 4.998685}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processinge14202e3-dd4b-4250-bf27-e2ce5efa0ffe\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 7.096508}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processing52166842-71e4-4a3f-86c8-42e76c44a74e\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 7.096508}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing52166842-71e4-4a3f-86c8-42e76c44a74e\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 4.764872}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processingae3cf501-dda1-496c-816b-c9e59cff8a29\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 4.764872}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processingae3cf501-dda1-496c-816b-c9e59cff8a29\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 4.910174}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processing25322263-77ea-4cf9-9331-244b82e493e2\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 4.910174}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing25322263-77ea-4cf9-9331-244b82e493e2\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 8.398395}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processing799bab0e-e3a9-4cbe-84b6-5889fb2783b1\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 8.398395}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing799bab0e-e3a9-4cbe-84b6-5889fb2783b1\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 5.32816}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processing0312a267-bc51-4568-91fa-07cfc61d0472\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 5.32816}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"8940003826ee\", \"OllamaHost\": \"http://gpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing0312a267-bc51-4568-91fa-07cfc61d0472\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 4.686883}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"fb03db0137f1\", \"OllamaHost\": \"http://cpuworker1:11434\", \"Status\": \"Ready\", \"Message\": \"Model llama3.1 Downloaded\", \"Models\": [], \"LastQueryTime\": 0}\n",
      "Received message: {\"Host\": \"fb03db0137f1\", \"OllamaHost\": \"http://cpuworker1:11434\", \"Status\": \"Running\", \"Message\": \"Processing74ab5fe8-2e99-4937-9226-8c6c018da4ee\", \"Models\": [], \"LastQueryTime\": 0}\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"fb03db0137f1\", \"OllamaHost\": \"http://cpuworker1:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing74ab5fe8-2e99-4937-9226-8c6c018da4ee\", \"Models\": [], \"LastQueryTime\": 69.317696}\n",
      "Receiving messages from the queue...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mtimeout\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mtimeout\u001b[0m: The read operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mreceive_messages_from_queue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mreceive_messages_from_queue\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceiving messages from the queue...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Receive messages in a batch, you can specify max_message_count as needed\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m received_msgs \u001b[38;5;241m=\u001b[39m \u001b[43mreceiver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_message_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_wait_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m received_msgs:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Print the message payload\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(msg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\pythonenvs\\pyTorchCUDA\\lib\\site-packages\\azure\\servicebus\\_servicebus_receiver.py:702\u001b[0m, in \u001b[0;36mServiceBusReceiver.receive_messages\u001b[1;34m(self, max_message_count, max_wait_time)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe max_message_count must be greater than 0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    701\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns()\n\u001b[1;32m--> 702\u001b[0m messages: List[ServiceBusReceivedMessage] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_retryable_operation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_message_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_message_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation_requires_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    708\u001b[0m links \u001b[38;5;241m=\u001b[39m get_receive_links(messages)\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m receive_trace_context_manager(\u001b[38;5;28mself\u001b[39m, links\u001b[38;5;241m=\u001b[39mlinks, start_time\u001b[38;5;241m=\u001b[39mstart_time):\n",
      "File \u001b[1;32mc:\\pythonenvs\\pyTorchCUDA\\lib\\site-packages\\azure\\servicebus\\_base_handler.py:411\u001b[0m, in \u001b[0;36mBaseHandler._do_retryable_operation\u001b[1;34m(self, operation, timeout, **kwargs)\u001b[0m\n\u001b[0;32m    409\u001b[0m         remaining_timeout \u001b[38;5;241m=\u001b[39m abs_timeout_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    410\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[1;32m--> 411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m operation(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\pythonenvs\\pyTorchCUDA\\lib\\site-packages\\azure\\servicebus\\_servicebus_receiver.py:461\u001b[0m, in \u001b[0;36mServiceBusReceiver._receive\u001b[1;34m(self, max_message_count, timeout)\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    460\u001b[0m before \u001b[38;5;241m=\u001b[39m received_messages_queue\u001b[38;5;241m.\u001b[39mqsize()\n\u001b[1;32m--> 461\u001b[0m receiving \u001b[38;5;241m=\u001b[39m \u001b[43mamqp_receive_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_work\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    462\u001b[0m received \u001b[38;5;241m=\u001b[39m received_messages_queue\u001b[38;5;241m.\u001b[39mqsize() \u001b[38;5;241m-\u001b[39m before\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m first_message_received\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m received_messages_queue\u001b[38;5;241m.\u001b[39mqsize() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m received \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    467\u001b[0m ):\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;66;03m# first message(s) received, continue receiving for some time\u001b[39;00m\n",
      "File \u001b[1;32mc:\\pythonenvs\\pyTorchCUDA\\lib\\site-packages\\azure\\servicebus\\_pyamqp\\client.py:427\u001b[0m, in \u001b[0;36mAMQPClient.do_work\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_ready():\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_run(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\pythonenvs\\pyTorchCUDA\\lib\\site-packages\\azure\\servicebus\\_pyamqp\\client.py:855\u001b[0m, in \u001b[0;36mReceiveClient._client_run\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_link\u001b[38;5;241m.\u001b[39mcurrent_link_credit \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    854\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_link\u001b[38;5;241m.\u001b[39mflow(link_credit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_link_credit)\n\u001b[1;32m--> 855\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mlisten(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket_timeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    856\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    857\u001b[0m     _logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimeout reached, closing receiver.\u001b[39m\u001b[38;5;124m\"\u001b[39m, extra\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_trace_params)\n",
      "File \u001b[1;32mc:\\pythonenvs\\pyTorchCUDA\\lib\\site-packages\\azure\\servicebus\\_pyamqp\\_connection.py:779\u001b[0m, in \u001b[0;36mConnection.listen\u001b[1;34m(self, wait, batch, **kwargs)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch):\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_read():\n\u001b[1;32m--> 779\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_frame(wait\u001b[38;5;241m=\u001b[39mwait, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    780\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\pythonenvs\\pyTorchCUDA\\lib\\site-packages\\azure\\servicebus\\_pyamqp\\_connection.py:286\u001b[0m, in \u001b[0;36mConnection._read_frame\u001b[1;34m(self, wait, **kwargs)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;66;03m# Since we use `sock.settimeout()` in the transport for reading/writing, that acts as a\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# \"block with timeout\" when we pass in a timeout value. If `wait` is float value, then\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# timeout was set during socket init.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:    \u001b[38;5;66;03m# wait is float/int/False\u001b[39;00m\n\u001b[1;32m--> 286\u001b[0m     new_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mreceive_frame(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mblock():\n",
      "File \u001b[1;32mc:\\pythonenvs\\pyTorchCUDA\\lib\\site-packages\\azure\\servicebus\\_pyamqp\\_transport.py:475\u001b[0m, in \u001b[0;36m_AbstractTransport.receive_frame\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreceive_frame\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 475\u001b[0m         header, channel, payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m payload:\n\u001b[0;32m    477\u001b[0m             decoded \u001b[38;5;241m=\u001b[39m decode_empty_frame(header)\n",
      "File \u001b[1;32mc:\\pythonenvs\\pyTorchCUDA\\lib\\site-packages\\azure\\servicebus\\_pyamqp\\_transport.py:413\u001b[0m, in \u001b[0;36m_AbstractTransport.read\u001b[1;34m(self, verify_frame_type)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    412\u001b[0m     frame_header \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(\u001b[38;5;28mbytearray\u001b[39m(\u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m--> 413\u001b[0m     read_frame_buffer\u001b[38;5;241m.\u001b[39mwrite(\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_header\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[0;32m    415\u001b[0m     channel \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>H\u001b[39m\u001b[38;5;124m\"\u001b[39m, frame_header[\u001b[38;5;241m6\u001b[39m:])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    416\u001b[0m     size \u001b[38;5;241m=\u001b[39m frame_header[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m4\u001b[39m]\n",
      "File \u001b[1;32mc:\\pythonenvs\\pyTorchCUDA\\lib\\site-packages\\azure\\servicebus\\_pyamqp\\_transport.py:624\u001b[0m, in \u001b[0;36mSSLTransport._read\u001b[1;34m(self, n, initial, buffer, _errnos)\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m toread:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 624\u001b[0m         nbytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mview\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    626\u001b[0m         \u001b[38;5;66;03m# ssl.sock.read may cause a SSLerror without errno\u001b[39;00m\n\u001b[0;32m    627\u001b[0m         \u001b[38;5;66;03m# http://bugs.python.org/issue10272\u001b[39;00m\n\u001b[0;32m    628\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, SSLError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimed out\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(exc):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    receive_messages_from_queue()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorchCUDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
