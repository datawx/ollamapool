{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLLAMAPool WebUI Monitoring\n",
    "import os\n",
    "EndPoint_NodeStatus=os.environ.get('EndPoint_NodeStatus')\n",
    "\n",
    "#Assert if the environment variables are set\n",
    "if EndPoint_NodeStatus is None:\n",
    "    raise ValueError(\"EndPoint_NodeStatus is not set\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trimmed down NodeStatus class\n",
    "import json\n",
    "class NodeStatus():\n",
    "    \n",
    "    def from_json(self,json_str):\n",
    "        self.__dict__=json.loads(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Message Handling Loop\n",
    "import time\n",
    "from typing import List\n",
    "from azure.servicebus import ServiceBusClient, ServiceBusMessage\n",
    "\n",
    "# Replace with your connection string and queue name\n",
    "queue_name = \"node-status\"\n",
    "hosts_Status={}\n",
    "\n",
    "# Create a Service Bus client\n",
    "servicebus_client = ServiceBusClient.from_connection_string(conn_str=EndPoint_NodeStatus)\n",
    "\n",
    "def receive_messages_from_queue():\n",
    "    # Create a receiver for the queue\n",
    "    with servicebus_client.get_queue_receiver(queue_name=queue_name) as receiver:\n",
    "        print(\"Receiving messages from the queue...\")\n",
    "        \n",
    "        # Receive messages in a batch, you can specify max_message_count as needed\n",
    "        received_msgs = receiver.receive_messages(max_message_count=20, max_wait_time=30)\n",
    "\n",
    "        for msg in received_msgs:\n",
    "            # Print the message payload\n",
    "            print(f\"Received message: {str(msg)}\")\n",
    "            \n",
    "            #Update node status\n",
    "            nodeStatus=NodeStatus()\n",
    "            nodeStatus.from_json(str(msg))\n",
    "            hosts_Status[nodeStatus.Host]=nodeStatus\n",
    "            print(f\"Hosts Status: {hosts_Status}\")\n",
    "            #print(nodeStatus.to_json())\n",
    "            \n",
    "            # Accept the message to remove it from the queue\n",
    "            receiver.complete_message(msg)\n",
    "            \n",
    "\n",
    "#receive_messages_from_queue()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.1.102:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "#Web Dash\n",
    "from flask import Flask, render_template\n",
    "from threading import Thread\n",
    "import json\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Example hosts_Status dictionary\n",
    "# hosts_Status = {\n",
    "#     \"host1\": {\"Host\": \"host1\", \"OllamaHost\": \"ollama1\", \"Status\": \"Online\", \"Message\": \"All good\", \"Models\": [\"model1\", \"model2\"], \"LastQueryTime\": 1234567890},\n",
    "#     \"host2\": {\"Host\": \"host2\", \"OllamaHost\": \"ollama2\", \"Status\": \"Offline\", \"Message\": \"Error\", \"Models\": [\"model3\"], \"LastQueryTime\": 1234567890}\n",
    "# }\n",
    "\n",
    "@app.route('/')\n",
    "def show_hosts():\n",
    "    return render_template('hosts.html', hosts_status=hosts_Status)\n",
    "\n",
    "# Thread to run Flask in Jupyter\n",
    "def run_app():\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "\n",
    "# Running Flask in a separate thread\n",
    "flask_thread = Thread(target=run_app)\n",
    "flask_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"Initializing...\", \"Message\": \"\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 0}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757B355310>}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"Initializing...\", \"Message\": \"\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 0}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757B331730>}\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"Initializing...\", \"Message\": \"\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 0}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757B365760>}\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"Initializing...\", \"Message\": \"\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 0}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757B355580>}\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"HERP\", \"Message\": \"HERP DERP\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 0}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757B355670>}\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"Running\", \"Message\": \"Processing570321b8-18c0-4d26-9ee2-fcb6f19bb67e\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 0}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757B3310A0>}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing570321b8-18c0-4d26-9ee2-fcb6f19bb67e\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 6.375092}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757B35EC70>}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"Running\", \"Message\": \"Processingd61f8b81-6f5d-479f-bea6-75b93c5fd5a3\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 6.375092}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757B3656D0>}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processingd61f8b81-6f5d-479f-bea6-75b93c5fd5a3\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 5.365568}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757B395190>}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"Running\", \"Message\": \"Processing42bf345a-8e01-45bf-b18d-c3282aa87cb0\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 5.365568}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757B395BB0>}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing42bf345a-8e01-45bf-b18d-c3282aa87cb0\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 6.565203}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757B3659D0>}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"Running\", \"Message\": \"Processing27fb4114-9e26-485c-bbda-c76f20efc9c8\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 6.565203}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757B395C70>}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing27fb4114-9e26-485c-bbda-c76f20efc9c8\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 5.211184}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757B35E3A0>}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"Running\", \"Message\": \"Processingd9d3ea32-4582-4fbe-b115-0e83f2358f75\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 5.211184}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757B355C40>}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processingd9d3ea32-4582-4fbe-b115-0e83f2358f75\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 5.160641}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757B3956D0>}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"Running\", \"Message\": \"Processing6e85af22-ec03-4ca5-bbe0-258bdcd22e15\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 5.160641}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757B3997F0>}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing6e85af22-ec03-4ca5-bbe0-258bdcd22e15\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 4.691518}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757C371130>}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"Running\", \"Message\": \"Processing62a59855-89e6-48d4-a7ad-85149810267a\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 4.691518}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757B399820>}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing62a59855-89e6-48d4-a7ad-85149810267a\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 5.481859}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757B395370>}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"Running\", \"Message\": \"Processing515980ea-b58d-4a9d-808d-4953e8ce8ed0\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 5.481859}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757B3958E0>}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing515980ea-b58d-4a9d-808d-4953e8ce8ed0\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 5.926494}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757B3311C0>}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"Running\", \"Message\": \"Processing9abc2e7b-7c2f-4cd0-8e59-ec943d68fce3\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 5.926494}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757B3550A0>}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing9abc2e7b-7c2f-4cd0-8e59-ec943d68fce3\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 6.623853}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757B365F40>}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"Running\", \"Message\": \"Processing942d5652-de58-46fd-a7f7-021f44b2f631\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 6.623853}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757B35ECD0>}\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"Finsihed\", \"Message\": \"Processing942d5652-de58-46fd-a7f7-021f44b2f631\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 6.299696}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757B35E850>}\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Receiving messages from the queue...\n",
      "Received message: {\"Host\": \"Euclid\", \"OllamaHost\": \"http://localhost:11434\", \"Status\": \"Shutdown\", \"Message\": \"Shut Down Complete\", \"Models\": [\"llama3.1:latest\"], \"LastQueryTime\": 6.299696}\n",
      "Hosts Status: {'Euclid': <__main__.NodeStatus object at 0x000002757B365BB0>}\n",
      "Receiving messages from the queue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.1.102 - - [23/Sep/2024 12:18:54] \"GET /?djdjd HTTP/1.1\" 200 -\n",
      "192.168.1.102 - - [23/Sep/2024 12:18:56] \"GET /?djdjd HTTP/1.1\" 200 -\n",
      "192.168.1.102 - - [23/Sep/2024 12:19:43] \"GET /?djdjd HTTP/1.1\" 200 -\n",
      "192.168.1.102 - - [23/Sep/2024 12:19:46] \"GET /?djdjd HTTP/1.1\" 200 -\n",
      "192.168.1.102 - - [23/Sep/2024 12:19:47] \"GET /?djdjd HTTP/1.1\" 200 -\n",
      "192.168.1.102 - - [23/Sep/2024 12:19:48] \"GET /?djdjd HTTP/1.1\" 200 -\n",
      "192.168.1.102 - - [23/Sep/2024 12:19:49] \"GET /?djdjd HTTP/1.1\" 200 -\n",
      "192.168.1.102 - - [23/Sep/2024 12:19:51] \"GET /?djdjd HTTP/1.1\" 200 -\n",
      "192.168.1.102 - - [23/Sep/2024 12:19:52] \"GET /?djdjd HTTP/1.1\" 200 -\n",
      "192.168.1.102 - - [23/Sep/2024 12:19:54] \"GET /?djdjd HTTP/1.1\" 200 -\n",
      "192.168.1.102 - - [23/Sep/2024 12:19:55] \"GET /?djdjd HTTP/1.1\" 200 -\n",
      "192.168.1.102 - - [23/Sep/2024 12:19:56] \"GET /?djdjd HTTP/1.1\" 200 -\n",
      "192.168.1.102 - - [23/Sep/2024 12:20:03] \"GET /?djdjd HTTP/1.1\" 200 -\n",
      "192.168.1.102 - - [23/Sep/2024 12:20:03] \"GET /?djdjd HTTP/1.1\" 200 -\n",
      "192.168.1.102 - - [23/Sep/2024 12:20:04] \"GET /?djdjd HTTP/1.1\" 200 -\n",
      "192.168.1.102 - - [23/Sep/2024 12:26:57] \"GET /?djdjd HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    receive_messages_from_queue()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorchCUDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
